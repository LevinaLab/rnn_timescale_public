{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-08-08T15:45:03.038209090Z",
     "start_time": "2023-08-08T15:45:01.948248156Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def generate_binary_sequence(M):\n",
    "    # return (torch.rand(M) < torch.rand(1)) * 1.\n",
    "    return ((torch.rand(M) < 0.5) * 1.)*2 - 1\n",
    "\n",
    "def make_batch_Nbit_pair_parity(Ns, M, bs):\n",
    "    with torch.no_grad():\n",
    "        sequences = [generate_binary_sequence(M).unsqueeze(-1) for i in range(bs)]\n",
    "\n",
    "        labels = [torch.stack([get_parity(s, N) for s in sequences]) for N in Ns]\n",
    "\n",
    "    return torch.stack(sequences), labels\n",
    "\n",
    "def get_parity(vec, N):\n",
    "    return  (((vec + 1)/2)[-N:].sum() % 2).long()\n",
    "    # return (vec[-N:].sum() % 2).long()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-08T15:45:03.045492979Z",
     "start_time": "2023-08-08T15:45:03.040932645Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "\n",
    "class LSTM_custom(nn.Module):\n",
    "    '''\n",
    "    Custom LSTM class so that we can make RNNs with layers with different sizes,\n",
    "    and also to save hidden_layer states through time.\n",
    "\n",
    "     Parameters\n",
    "    -----------\n",
    "    input_size: int\n",
    "        size of input (it has been always 1)\n",
    "    net_size: list\n",
    "        list of number of neurons per layer (size larger than 1 it is for a multi layer network)\n",
    "    num_classes: int\n",
    "        number of classes for classification\n",
    "    bias: boolean, default True\n",
    "        if we include bias terms\n",
    "    num_readout_heads: int\n",
    "        number of outputs\n",
    "    '''\n",
    "\n",
    "    def __init__(self, input_size=1,\n",
    "                 hidden_size=100,\n",
    "                 num_layers=1,\n",
    "                 num_classes=2,\n",
    "                 bias=True,\n",
    "                 num_readout_heads=1,\n",
    "                 ):\n",
    "        super(LSTM_custom, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_classes = num_classes\n",
    "        self.bias = bias\n",
    "        self.num_readout_heads = num_readout_heads\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, bias, batch_first=True)\n",
    "\n",
    "        self.fc = [nn.Linear(hidden_size, num_classes, bias=bias) for i in range(num_readout_heads)]\n",
    "\n",
    "        self.modulelist = nn.ModuleList(self.fc)\n",
    "\n",
    "\n",
    "    def forward(self, data, h0=None, c0=None):\n",
    "        '''\n",
    "        input: data [batch_size, sequence length, input_features]\n",
    "        output:\n",
    "                h0:\n",
    "                readout: readout from the fc layers at the end,\n",
    "                    shape=[batch_size, self.num_classes],\n",
    "        '''\n",
    "        if h0 is None:\n",
    "            h0 = torch.zeros(self.num_layers, data.size(0), self.hidden_size).to(device)\n",
    "        if c0 is None:\n",
    "            c0 = torch.zeros(self.num_layers, data.size(0), self.hidden_size).to(device)\n",
    "        output, (h_n, c_n) = self.lstm(data, (h0, c0))\n",
    "\n",
    "        # readout = [self.fc[i](h_n[-1]) for i in range(self.num_readout_heads)]\n",
    "        readout = [self.fc[i](output[:, -1, :]) for i in range(self.num_readout_heads)]\n",
    "\n",
    "\n",
    "        return output, readout\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-08T15:45:03.932829742Z",
     "start_time": "2023-08-08T15:45:03.930387592Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "EPOCHS = 1000\n",
    "INPUT_SIZE = 1\n",
    "\n",
    "NET_SIZE = 500\n",
    "NUM_LAYERS = 1\n",
    "NUM_CLASSES = 2\n",
    "BIAS = True\n",
    "\n",
    "NUM_READOUT_HEADS = 100\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "TRAINING_STEPS = 300\n",
    "TEST_STEPS = 50\n",
    "\n",
    "lstm = LSTM_custom(\n",
    "    input_size=INPUT_SIZE,\n",
    "    hidden_size=NET_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    bias=BIAS,\n",
    "    num_readout_heads=NUM_READOUT_HEADS\n",
    ").to(device)\n",
    "\n",
    "learning_rate = 1e-3\n",
    "# optimizer = torch.optim.SGD(lstm.parameters(), lr=learning_rate, momentum=0.1, nesterov=True)\n",
    "optimizer =  torch.optim.Adam(lstm.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-08T15:45:09.031042020Z",
     "start_time": "2023-08-08T15:45:07.726401209Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# set the forget gate params to 1. to NOT forget stuff at first\n",
    "lstm.lstm.weight_ih_l0.data[NET_SIZE:NET_SIZE*2, :] = torch.ones_like(lstm.lstm.weight_ih_l0.data[NET_SIZE:NET_SIZE*2, :])\n",
    "lstm.lstm.weight_hh_l0.data[NET_SIZE:NET_SIZE*2, :] = torch.ones_like(lstm.lstm.weight_hh_l0.data[NET_SIZE:NET_SIZE*2, :])\n",
    "lstm.lstm.bias_ih_l0.data[NET_SIZE:NET_SIZE*2] = torch.ones_like(lstm.lstm.bias_ih_l0.data[NET_SIZE:NET_SIZE*2])\n",
    "lstm.lstm.bias_hh_l0.data[NET_SIZE:NET_SIZE*2] = torch.ones_like(lstm.lstm.bias_hh_l0.data[NET_SIZE:NET_SIZE*2])\n",
    "\n",
    "lstm.lstm.flatten_parameters()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-08-08T15:45:12.317785402Z",
     "start_time": "2023-08-08T15:45:12.311820585Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Step [300/300], Loss: 0.0000, Accuracy 100.0000  %\n",
      "({N}, accuracy):\n",
      "(2, 100.0000)\n",
      "\n",
      "Finishing training for N = [2]...\n",
      "N = 2, 3\n",
      "Epoch [2/100], Step [300/300], Loss: 0.0001, Accuracy 100.0000  %\n",
      "({N}, accuracy):\n",
      "(2, 100.0000)\n",
      "(3, 100.0000)\n",
      "\n",
      "Finishing training for N = [2, 3]...\n",
      "N = 2, 4\n",
      "Epoch [3/100], Step [300/300], Loss: 0.0003, Accuracy 100.0000  %\n",
      "({N}, accuracy):\n",
      "(2, 100.0000)\n",
      "(3, 100.0000)\n",
      "(4, 100.0000)\n",
      "\n",
      "Finishing training for N = [2, 3, 4]...\n",
      "N = 2, 5\n",
      "Epoch [4/100], Step [300/300], Loss: 0.0004, Accuracy 100.0000  %\n",
      "({N}, accuracy):\n",
      "(2, 100.0000)\n",
      "(3, 100.0000)\n",
      "(4, 100.0000)\n",
      "(5, 100.0000)\n",
      "\n",
      "Finishing training for N = [2, 3, 4, 5]...\n",
      "N = 2, 6\n",
      "Epoch [5/100], Step [300/300], Loss: 0.0004, Accuracy 100.0000  %\n",
      "({N}, accuracy):\n",
      "(2, 100.0000)\n",
      "(3, 100.0000)\n",
      "(4, 100.0000)\n",
      "(5, 100.0000)\n",
      "(6, 100.0000)\n",
      "\n",
      "Finishing training for N = [2, 3, 4, 5, 6]...\n",
      "N = 2, 7\n",
      "Epoch [6/100], Step [300/300], Loss: 0.0005, Accuracy 100.0000  %\n",
      "({N}, accuracy):\n",
      "(2, 100.0000)\n",
      "(3, 100.0000)\n",
      "(4, 100.0000)\n",
      "(5, 100.0000)\n",
      "(6, 100.0000)\n",
      "(7, 100.0000)\n",
      "\n",
      "Finishing training for N = [2, 3, 4, 5, 6, 7]...\n",
      "N = 2, 8\n",
      "Epoch [7/100], Step [300/300], Loss: 0.0006, Accuracy 100.0000  %\n",
      "({N}, accuracy):\n",
      "(2, 100.0000)\n",
      "(3, 100.0000)\n",
      "(4, 100.0000)\n",
      "(5, 100.0000)\n",
      "(6, 100.0000)\n",
      "(7, 100.0000)\n",
      "(8, 100.0000)\n",
      "\n",
      "Finishing training for N = [2, 3, 4, 5, 6, 7, 8]...\n",
      "N = 2, 9\n",
      "Epoch [8/100], Step [300/300], Loss: 0.0007, Accuracy 100.0000  %\n",
      "({N}, accuracy):\n",
      "(2, 100.0000)\n",
      "(3, 100.0000)\n",
      "(4, 100.0000)\n",
      "(5, 100.0000)\n",
      "(6, 100.0000)\n",
      "(7, 100.0000)\n",
      "(8, 100.0000)\n",
      "(9, 100.0000)\n",
      "\n",
      "Finishing training for N = [2, 3, 4, 5, 6, 7, 8, 9]...\n",
      "N = 2, 10\n",
      "Epoch [9/100], Step [300/300], Loss: 0.0007, Accuracy 100.0000  %\n",
      "({N}, accuracy):\n",
      "(2, 100.0000)\n",
      "(3, 100.0000)\n",
      "(4, 100.0000)\n",
      "(5, 100.0000)\n",
      "(6, 100.0000)\n",
      "(7, 100.0000)\n",
      "(8, 100.0000)\n",
      "(9, 100.0000)\n",
      "(10, 100.0000)\n",
      "\n",
      "Finishing training for N = [2, 3, 4, 5, 6, 7, 8, 9, 10]...\n",
      "N = 2, 11\n",
      "Epoch [10/100], Step [300/300], Loss: 0.0006, Accuracy 100.0000  %\n",
      "({N}, accuracy):\n",
      "(2, 100.0000)\n",
      "(3, 100.0000)\n",
      "(4, 100.0000)\n",
      "(5, 100.0000)\n",
      "(6, 100.0000)\n",
      "(7, 100.0000)\n",
      "(8, 100.0000)\n",
      "(9, 100.0000)\n",
      "(10, 100.0000)\n",
      "(11, 100.0000)\n",
      "\n",
      "Finishing training for N = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11]...\n",
      "N = 2, 12\n",
      "Epoch [11/100], Step [300/300], Loss: 0.0008, Accuracy 100.0000  %\n",
      "({N}, accuracy):\n",
      "(2, 100.0000)\n",
      "(3, 100.0000)\n",
      "(4, 100.0000)\n",
      "(5, 100.0000)\n",
      "(6, 100.0000)\n",
      "(7, 100.0000)\n",
      "(8, 100.0000)\n",
      "(9, 100.0000)\n",
      "(10, 100.0000)\n",
      "(11, 100.0000)\n",
      "(12, 100.0000)\n",
      "\n",
      "Finishing training for N = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]...\n",
      "N = 2, 13\n",
      "Epoch [12/100], Step [300/300], Loss: 0.0014, Accuracy 100.0000  %\n",
      "({N}, accuracy):\n",
      "(2, 100.0000)\n",
      "(3, 100.0000)\n",
      "(4, 100.0000)\n",
      "(5, 100.0000)\n",
      "(6, 100.0000)\n",
      "(7, 100.0000)\n",
      "(8, 100.0000)\n",
      "(9, 100.0000)\n",
      "(10, 100.0000)\n",
      "(11, 100.0000)\n",
      "(12, 100.0000)\n",
      "(13, 100.0000)\n",
      "\n",
      "Finishing training for N = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]...\n",
      "N = 2, 14\n",
      "Epoch [13/100], Step [300/300], Loss: 0.0663, Accuracy 99.9832  %\n",
      "({N}, accuracy):\n",
      "(2, 100.0000)\n",
      "(3, 100.0000)\n",
      "(4, 100.0000)\n",
      "(5, 100.0000)\n",
      "(6, 99.9688)\n",
      "(7, 100.0000)\n",
      "(8, 99.9688)\n",
      "(9, 100.0000)\n",
      "(10, 100.0000)\n",
      "(11, 99.9688)\n",
      "(12, 99.9688)\n",
      "(13, 99.9375)\n",
      "(14, 99.9688)\n",
      "\n",
      "Finishing training for N = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]...\n",
      "N = 2, 15\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [9], line 86\u001B[0m\n\u001B[1;32m     83\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m stats\n\u001B[1;32m     85\u001B[0m NHEADS \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m---> 86\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlstm\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mNs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn [9], line 18\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(num_epochs, model, Ns)\u001B[0m\n\u001B[1;32m     14\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m     16\u001B[0m M \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39mrandint(M_MIN, M_MAX)\n\u001B[0;32m---> 18\u001B[0m sequences, labels \u001B[38;5;241m=\u001B[39m \u001B[43mmake_batch_Nbit_pair_parity\u001B[49m\u001B[43m(\u001B[49m\u001B[43mNs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mM\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mBATCH_SIZE\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     19\u001B[0m sequences \u001B[38;5;241m=\u001B[39m sequences\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m     20\u001B[0m labels \u001B[38;5;241m=\u001B[39m [l\u001B[38;5;241m.\u001B[39mto(device) \u001B[38;5;28;01mfor\u001B[39;00m l \u001B[38;5;129;01min\u001B[39;00m labels]\n",
      "Cell \u001B[0;32mIn [2], line 9\u001B[0m, in \u001B[0;36mmake_batch_Nbit_pair_parity\u001B[0;34m(Ns, M, bs)\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m      7\u001B[0m     sequences \u001B[38;5;241m=\u001B[39m [generate_binary_sequence(M)\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(bs)]\n\u001B[0;32m----> 9\u001B[0m     labels \u001B[38;5;241m=\u001B[39m [torch\u001B[38;5;241m.\u001B[39mstack([get_parity(s, N) \u001B[38;5;28;01mfor\u001B[39;00m s \u001B[38;5;129;01min\u001B[39;00m sequences]) \u001B[38;5;28;01mfor\u001B[39;00m N \u001B[38;5;129;01min\u001B[39;00m Ns]\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mstack(sequences), labels\n",
      "Cell \u001B[0;32mIn [2], line 9\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m      7\u001B[0m     sequences \u001B[38;5;241m=\u001B[39m [generate_binary_sequence(M)\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(bs)]\n\u001B[0;32m----> 9\u001B[0m     labels \u001B[38;5;241m=\u001B[39m [torch\u001B[38;5;241m.\u001B[39mstack([get_parity(s, N) \u001B[38;5;28;01mfor\u001B[39;00m s \u001B[38;5;129;01min\u001B[39;00m sequences]) \u001B[38;5;28;01mfor\u001B[39;00m N \u001B[38;5;129;01min\u001B[39;00m Ns]\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mstack(sequences), labels\n",
      "Cell \u001B[0;32mIn [2], line 9\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m      7\u001B[0m     sequences \u001B[38;5;241m=\u001B[39m [generate_binary_sequence(M)\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(bs)]\n\u001B[0;32m----> 9\u001B[0m     labels \u001B[38;5;241m=\u001B[39m [torch\u001B[38;5;241m.\u001B[39mstack([\u001B[43mget_parity\u001B[49m\u001B[43m(\u001B[49m\u001B[43ms\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mN\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m s \u001B[38;5;129;01min\u001B[39;00m sequences]) \u001B[38;5;28;01mfor\u001B[39;00m N \u001B[38;5;129;01min\u001B[39;00m Ns]\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mstack(sequences), labels\n",
      "Cell \u001B[0;32mIn [2], line 14\u001B[0m, in \u001B[0;36mget_parity\u001B[0;34m(vec, N)\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_parity\u001B[39m(vec, N):\n\u001B[0;32m---> 14\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m  (\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvec\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43mN\u001B[49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msum\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m%\u001B[39m \u001B[38;5;241m2\u001B[39m)\u001B[38;5;241m.\u001B[39mlong()\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "def train(num_epochs, model, Ns):\n",
    "    M_MIN = Ns[-1] + 2\n",
    "    M_MAX = M_MIN + 3 * Ns[-1]\n",
    "\n",
    "    # stats\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "\n",
    "    # Train the model\n",
    "    total_step = TRAINING_STEPS\n",
    "    for epoch in range(num_epochs):\n",
    "        losses_step = []\n",
    "        for i in range(TRAINING_STEPS):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            M = np.random.randint(M_MIN, M_MAX)\n",
    "\n",
    "            sequences, labels = make_batch_Nbit_pair_parity(Ns, M, BATCH_SIZE)\n",
    "            sequences = sequences.to(device)\n",
    "            labels = [l.to(device) for l in labels]\n",
    "\n",
    "            # Forward pass\n",
    "            out, out_class = model(sequences)\n",
    "\n",
    "            # Backward and optimize\n",
    "            loss = 0.\n",
    "            for N_i in range(len(Ns)):\n",
    "                loss += criterion(out_class[N_i], labels[N_i])\n",
    "\n",
    "            losses_step.append(loss.item())\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0, norm_type=2)  # gradient clipping\n",
    "            optimizer.step()\n",
    "        losses.append(np.mean(losses_step))\n",
    "\n",
    "        # Test and measure accuracy\n",
    "        correct_N = np.zeros_like(Ns)\n",
    "        total = 0\n",
    "        for j in range(TEST_STEPS):\n",
    "            with torch.no_grad():\n",
    "                M = np.random.randint(M_MIN, M_MAX)\n",
    "\n",
    "                sequences, labels = make_batch_Nbit_pair_parity(Ns, M, BATCH_SIZE)\n",
    "                sequences = sequences.to(device)\n",
    "                labels = [l.to(device) for l in labels]\n",
    "\n",
    "                out, out_class = model(sequences)\n",
    "\n",
    "                for N_i in range(len(Ns)):\n",
    "                    predicted = torch.max(out_class[N_i], 1)[1]\n",
    "\n",
    "                    correct_N[N_i] += (predicted == labels[N_i]).sum()\n",
    "                    total += labels[N_i].size(0)\n",
    "\n",
    "        accuracy = 100 * correct_N / float(total) * len(Ns)\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "        print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy {:.4f}  %'\n",
    "              .format(epoch + 1, num_epochs, i + 1, total_step, losses[-1], np.mean(accuracy)), flush=True)\n",
    "        print('({N}, accuracy):\\n' + ''.join([f'({Ns[i]}, {accuracy[i]:.4f})\\n' for i in range(len(Ns))]), flush=True)\n",
    "\n",
    "        stats = {'loss': losses,\n",
    "                 'accuracy': accuracies}\n",
    "        # np.save(f'{MAIN_DIR}/{EXPERIMENT_NAME}/stats.npy', stats)\n",
    "\n",
    "        # curriculum stuff + save\n",
    "        if np.mean(accuracy) > 98.:\n",
    "            if accuracy[-1] > 98.:\n",
    "                print(f'Finishing training for N = ' + str(Ns) + '...', flush=True)\n",
    "\n",
    "                # append new curriculum\n",
    "                # multi-head\n",
    "                Ns = Ns + [Ns[-1] + 1 + i for i in range(NHEADS)]\n",
    "                \n",
    "                # single-head\n",
    "                # Ns = [Ns[-1] + 1]\n",
    "\n",
    "                M_MIN = Ns[-1] + 2\n",
    "                M_MAX = M_MIN + 3 * Ns[-1]\n",
    "                print(f'N = {Ns[0]}, {Ns[-1]}', flush=True)\n",
    "\n",
    "\n",
    "    return stats\n",
    "\n",
    "NHEADS = 1\n",
    "train(num_epochs=100, model=lstm, Ns=[2])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-08-08T16:19:20.581207710Z",
     "start_time": "2023-08-08T16:16:52.030069147Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-08-08T15:44:53.062496574Z",
     "start_time": "2023-08-08T15:44:53.062374586Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
